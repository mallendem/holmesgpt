name: Run eval regression tests

on:
  pull_request:
    branches: ["*"]
  push:
    branches: [master]
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to comment on (auto-detects from branch if empty)'
        required: false
        default: ''
      model:
        description: 'Model(s) to test. Examples: gpt-4.1, gpt-4o, anthropic/claude-sonnet-4-20250514, azure/gpt-4.1, bedrock/anthropic.claude-sonnet-4-20250514-v1:0, gemini/gemini-1.5-pro, vertex_ai/claude-3-5-sonnet, ollama/llama2. Comma-separated for multiple.'
        required: false
        default: ''
      markers:
        description: 'Pytest markers (e.g., regression, easy, logs). Combined with "llm and". Leave empty for all LLM tests.'
        required: false
        default: ''
      filter:
        description: 'Pytest -k filter (e.g., 09_crashpod, checkout)'
        required: false
        default: ''
      iterations:
        description: 'Number of iterations (1-10)'
        required: false
        default: '1'
      is_pr_trusted:
        description: 'Allow running on fork PRs (use with caution - grants fork code access to secrets)'
        required: false
        default: 'false'
        type: boolean
  issue_comment:
    types: [created]

permissions:
  pull-requests: write
  contents: read
  issues: write

jobs:
  # Handle /list command - outputs available eval names
  list_evals:
    runs-on: ubuntu-latest
    if: github.event_name == 'issue_comment' && github.event.issue.pull_request && startsWith(github.event.comment.body, '/list')
    steps:
      - name: Check permissions and fork status
        uses: actions/github-script@v7
        with:
          script: |
            // Check commenter has write access
            const association = context.payload.comment.author_association;
            if (!['OWNER', 'MEMBER', 'COLLABORATOR'].includes(association)) {
              core.setFailed(`Permission denied: ${association} cannot use /list (requires OWNER, MEMBER, or COLLABORATOR)`);
              return;
            }

            // SECURITY: Block /list on fork PRs (although tbh it should be safe)
            const pr = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number
            });
            // Treat orphaned PRs (deleted head repo) as forks
            const isFork = !pr.data.head.repo || pr.data.head.repo.full_name !== pr.data.base.repo.full_name;
            if (isFork) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: '‚ùå `/list` command is not available on fork PRs for security reasons.'
              });
              core.setFailed('Fork PRs cannot use /list command');
              return;
            }

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Post eval list
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Collect eval names from fixture directories
            const askHolmesPath = 'tests/llm/fixtures/test_ask_holmes';
            const investigatePath = 'tests/llm/fixtures/test_investigate';

            const getEvalNames = (dir) => {
              try {
                return fs.readdirSync(dir)
                  .filter(f => !f.startsWith('.'))
                  .sort()
                  .map(f => `\`${f}\``)
                  .join(', ');
              } catch (e) {
                return '_(none found)_';
              }
            };

            const askHolmesEvals = getEvalNames(askHolmesPath);
            const investigateEvals = getEvalNames(investigatePath);

            const body = `## üìã Available Eval Names\n\n` +
              `Use these with \`filter:\` in your \`/eval\` command.\n\n` +
              `**test_ask_holmes:**\n${askHolmesEvals}\n\n` +
              `**test_investigate:**\n${investigateEvals}`;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });

  llm_evals:
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'pull_request' ||
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'issue_comment' && github.event.issue.pull_request && (startsWith(github.event.comment.body, '/eval') || startsWith(github.event.comment.body, '/rerun')))

    # Serialize automated regression runs per PR to avoid race conditions on persistent comment
    # Manual runs (/eval, workflow_dispatch) get unique groups so they can run in parallel
    concurrency:
      group: ${{ github.event_name == 'pull_request' && format('eval-auto-pr-{0}', github.event.pull_request.number) || format('eval-{0}', github.run_id) }}
      cancel-in-progress: false

    steps:
      - name: Handle /eval comment
        id: eval-comment
        if: github.event_name == 'issue_comment'
        uses: actions/github-script@v7
        with:
          script: |
            // Check commenter has write access
            const association = context.payload.comment.author_association;
            if (!['OWNER', 'MEMBER', 'COLLABORATOR'].includes(association)) {
              core.setFailed(`Permission denied: ${association} cannot trigger /eval (requires OWNER, MEMBER, or COLLABORATOR)`);
              return;
            }

            // Get PR info for fork detection
            const pr = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number
            });

            let body = context.payload.comment.body;

            // SECURITY: Check for trusted flag on fork PRs
            // Treat orphaned PRs (deleted head repo) as forks
            const isFork = !pr.data.head.repo || pr.data.head.repo.full_name !== pr.data.base.repo.full_name;
            if (isFork) {
              // Check if trusted: true is in the comment (parsed early, before checkout)
              const trustedMatch = body.match(/^trusted\s*:\s*true\s*$/im);
              const isTrusted = trustedMatch !== null;

              if (!isTrusted) {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: '‚ö†Ô∏è `/eval` and `/rerun` on fork PRs require explicit trust confirmation.\n\n' +
                        'Running evals grants the fork code access to secrets. If you\'ve reviewed the code, add `trusted: true`:\n\n' +
                        '```\n/eval\ntrusted: true\nmarkers: regression\n```\n\n' +
                        'Or use `workflow_dispatch` with `is_pr_trusted: true` from the Actions UI.'
                });
                core.setFailed('Fork PRs require trusted: true to run /eval or /rerun');
                return;
              }
              core.warning(`Running /eval on trusted fork PR #${context.issue.number} - fork code will have access to secrets`);
            }
            let lastEvalCommentId = null;
            const isRerunCommand = body.trim().toLowerCase().startsWith('/rerun');

            // Handle /rerun command: find the last /eval comment on this PR
            if (isRerunCommand) {
              // Fetch all comments (paginate to handle PRs with 100+ comments)
              let allComments = [];
              let page = 1;
              while (true) {
                const response = await github.rest.issues.listComments({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  per_page: 100,
                  page: page
                });
                allComments = allComments.concat(response.data);
                if (response.data.length < 100) break;
                page++;
              }

              // Find the last /eval comment before this /rerun comment
              const evalComments = allComments
                .filter(c => c.id !== context.payload.comment.id && c.body.trim().toLowerCase().startsWith('/eval'))
                .sort((a, b) => new Date(b.created_at) - new Date(a.created_at));

              if (evalComments.length === 0) {
                core.setFailed('No previous /eval comment found on this PR. Use /eval instead.');
                return;
              }

              body = evalComments[0].body;
              lastEvalCommentId = evalComments[0].id;
              core.info(`Found last /eval comment: ${lastEvalCommentId}`);
            }

            core.setOutput('is_rerun_command', isRerunCommand.toString());
            core.setOutput('last_eval_comment_id', lastEvalCommentId ? lastEvalCommentId.toString() : '');
            core.setOutput('resolved_body', body);

            // Parse branch parameter from comment if specified (for cross-branch comparison)
            let targetBranch = '';
            for (const line of body.split('\n').slice(1)) {
              const colonIdx = line.indexOf(':');
              if (colonIdx === -1) continue;
              const key = line.slice(0, colonIdx).trim().toLowerCase();
              const value = line.slice(colonIdx + 1).trim();
              if (key === 'branch' && value && /^[a-zA-Z0-9_.\/-]*$/.test(value)) {
                targetBranch = value;
                break;
              }
            }
            core.setOutput('branch', targetBranch);

            core.setOutput('pr_sha', pr.data.head.sha);
            core.setOutput('pr_branch', pr.data.head.ref);

            // Add eyes reaction to indicate the comment was seen and is being processed
            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: context.payload.comment.id,
              content: 'eyes'
            });

      # SECURITY: Checkout to sibling directories so neither can overwrite the other.
      # PR code goes to ./code/, trusted helpers go to ./.trusted/
      - name: Checkout PR code
        if: github.event_name != 'issue_comment' || steps.eval-comment.outcome == 'success'
        uses: actions/checkout@v4
        with:
          # For /eval with branch: checkout the specified branch; otherwise checkout PR SHA or current ref
          ref: ${{ steps.eval-comment.outputs.branch || steps.eval-comment.outputs.pr_sha || github.ref }}
          path: code

      - name: Checkout trusted helpers from base branch
        if: github.event_name != 'issue_comment' || steps.eval-comment.outcome == 'success'
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.base.ref || 'master' }}
          sparse-checkout: .github/scripts
          path: .trusted

      - name: Determine eval parameters
        if: github.event_name != 'issue_comment' || steps.eval-comment.outcome == 'success'
        id: eval-params
        uses: actions/github-script@v7
        with:
          script: |
            // SECURITY: These whitelist regexes protect against shell injection by only
            // allowing safe characters. This works together with proper quoting in bash
            // (using env vars + "$VAR" syntax). If you modify these patterns or add new
            // parameters, ensure: (1) regex is a whitelist of safe chars, (2) values are
            // passed via env vars not inline, (3) bash references use double quotes.
            const patterns = {
              model: /^[a-zA-Z0-9_.,:\/-]*$/,
              markers: /^[a-zA-Z0-9_\- ()]*$/,
              filter: /^[a-zA-Z0-9_\-]*$/,
              iterations: /^[0-9]+$/,
              pr_number: /^[0-9]*$/,
              branch: /^[a-zA-Z0-9_.\/-]*$/
            };

            function validate(value, name) {
              if (!value) return '';
              if (patterns[name] && !patterns[name].test(value)) {
                core.setFailed(`Invalid ${name}: must match ${patterns[name]}`);
                return null;
              }
              return value;
            }

            function parseComment(body) {
              const params = { model: '', markers: '', filter: '', iterations: '1', branch: '' };
              const validKeys = new Set(Object.keys(params));
              // Aliases: marker -> markers, filters -> filter
              const keyAliases = { marker: 'markers', filters: 'filter' };
              for (const line of body.split('\n').slice(1)) {
                const colonIdx = line.indexOf(':');
                if (colonIdx === -1) continue;
                let key = line.slice(0, colonIdx).trim().toLowerCase();
                // Normalize aliases
                if (keyAliases[key]) key = keyAliases[key];
                // Strip leading/trailing quotes (users sometimes add them unnecessarily)
                const value = line.slice(colonIdx + 1).trim().replace(/^['"]|['"]$/g, '');
                if (validKeys.has(key) && value) params[key] = value;
              }
              return params;
            }

            let model, markers, filter, iterations, isManual, triggerSource, prNumber, triggeredBy, branch, displayBranch;
            const event = context.eventName;

            if (event === 'issue_comment') {
              // Use resolved body (handles /rerun command by using the last /eval comment's body)
              const resolvedBody = ${{ toJSON(steps.eval-comment.outputs.resolved_body) }} || context.payload.comment.body;
              const isRerunCommand = ${{ toJSON(steps.eval-comment.outputs.is_rerun_command) }} === 'true';
              const lastEvalCommentId = ${{ toJSON(steps.eval-comment.outputs.last_eval_comment_id) }} || '';

              const parsed = parseComment(resolvedBody);
              model = validate(parsed.model, 'model');
              markers = validate(parsed.markers, 'markers');
              filter = validate(parsed.filter, 'filter');
              iterations = validate(parsed.iterations, 'iterations');
              branch = validate(parsed.branch, 'branch');
              if ([model, markers, filter, iterations, branch].includes(null)) return;
              // displayBranch shows what branch we're testing: override branch or PR branch
              const prBranch = ${{ toJSON(steps.eval-comment.outputs.pr_branch) }} || '';
              displayBranch = branch || prBranch;
              isManual = true;

              // Build trigger source with links to both commands
              const currentCommentUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/pull/${context.issue.number}#issuecomment-${context.payload.comment.id}`;
              if (isRerunCommand && lastEvalCommentId) {
                const evalCommentUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/pull/${context.issue.number}#issuecomment-${lastEvalCommentId}`;
                triggerSource = `[/rerun](${currentCommentUrl}) ‚Üí [/eval](${evalCommentUrl})`;
              } else {
                triggerSource = branch ? `[/eval on branch \`${branch}\`](${currentCommentUrl})` : `[/eval comment](${currentCommentUrl})`;
              }

              prNumber = context.issue.number;
              triggeredBy = context.payload.comment.user.login;
            } else if (event === 'workflow_dispatch') {
              model = validate(${{ toJSON(github.event.inputs.model) }} || '', 'model');
              markers = validate(${{ toJSON(github.event.inputs.markers) }} || '', 'markers');
              filter = validate(${{ toJSON(github.event.inputs.filter) }} || '', 'filter');
              iterations = validate(${{ toJSON(github.event.inputs.iterations) }} || '', 'iterations');
              const prNum = validate(${{ toJSON(github.event.inputs.pr_number) }} || '', 'pr_number');
              const isPrTrusted = ${{ toJSON(github.event.inputs.is_pr_trusted) }} === 'true';
              branch = ''; // workflow_dispatch doesn't support branch override yet
              if ([model, markers, filter, iterations, prNum].includes(null)) return;
              isManual = true;
              triggerSource = 'workflow_dispatch';
              triggeredBy = context.actor;
              displayBranch = context.ref.replace('refs/heads/', '');

              // Auto-detect PR from branch if pr_number not provided
              if (prNum) {
                prNumber = parseInt(prNum, 10);
              } else {
                const currentBranch = context.ref.replace('refs/heads/', '');
                const prs = await github.rest.pulls.list({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  head: `${context.repo.owner}:${currentBranch}`,
                  state: 'open'
                });
                if (prs.data.length === 1) {
                  prNumber = prs.data[0].number;
                  core.info(`Auto-detected PR #${prNumber} from branch ${currentBranch}`);
                } else if (prs.data.length > 1) {
                  core.warning(`Multiple open PRs found for branch ${currentBranch}, please specify pr_number`);
                  prNumber = null;
                } else {
                  core.info(`No open PR found for branch ${currentBranch}`);
                  prNumber = null;
                }
              }

              // SECURITY: Check if this is a fork PR and block unless explicitly trusted
              if (prNumber) {
                const pr = await github.rest.pulls.get({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  pull_number: prNumber
                });
                // Treat orphaned PRs (deleted head repo) as forks
                const isFork = !pr.data.head.repo || pr.data.head.repo.full_name !== pr.data.base.repo.full_name;
                if (isFork && !isPrTrusted) {
                  core.setFailed(
                    `Fork PR #${prNumber} detected. Manual workflow_dispatch on fork PRs requires ` +
                    `is_pr_trusted=true (grants fork code access to secrets).`
                  );
                  return;
                }
                if (isFork && isPrTrusted) {
                  core.warning(`Running on trusted fork PR #${prNumber} - fork code will have access to secrets`);
                }
              }
            } else {
              model = '';
              markers = 'regression';
              filter = '';
              iterations = '1';
              branch = '';
              isManual = false;
              prNumber = event === 'pull_request' ? context.payload.pull_request.number : null;
              triggeredBy = '';
              // Build descriptive trigger source for automatic runs
              if (event === 'pull_request') {
                const sha = context.payload.pull_request.head.sha.substring(0, 7);
                const prBranch = context.payload.pull_request.head.ref;
                displayBranch = prBranch;
                triggerSource = `commit ${sha} on branch \`${prBranch}\``;
              } else if (event === 'push') {
                const sha = context.sha.substring(0, 7);
                const pushBranch = context.ref.replace('refs/heads/', '');
                displayBranch = pushBranch;
                triggerSource = `push to \`${pushBranch}\` (${sha})`;
              } else {
                displayBranch = '';
                triggerSource = 'automatic';
              }
            }

            // Apply defaults
            if (!model) model = ${{ toJSON(vars.MODEL) }} || '';
            // Only apply regression default for automatic triggers, not for manual /eval or workflow_dispatch
            if (!markers && !isManual) markers = 'regression';
            let iterNum = Math.min(Math.max(parseInt(iterations, 10) || 1, 1), 10);

            core.setOutput('model', model);
            core.setOutput('markers', markers);
            core.setOutput('filter', filter);
            core.setOutput('iterations', iterNum.toString());
            core.setOutput('is_manual', isManual.toString());
            core.setOutput('trigger_source', triggerSource);
            core.setOutput('pr_number', prNumber ? prNumber.toString() : '');
            core.setOutput('triggered_by', triggeredBy);
            core.setOutput('branch', branch);
            core.setOutput('display_branch', displayBranch || '');
            core.setOutput('marker_expr', markers ? `llm and (${markers})` : 'llm');
            core.setOutput('run_url', `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`);

            // Output all common params as JSON for DRY usage in subsequent steps
            core.setOutput('base_params', JSON.stringify({
              is_manual: isManual.toString(),
              trigger_source: triggerSource,
              model,
              markers,
              filter,
              iterations: iterNum.toString(),
              branch,
              display_branch: displayBranch || '',
              run_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              pr_number: prNumber ? prNumber.toString() : ''
            }));

      - name: Check if tests should run
        id: check-tests
        if: github.event_name != 'issue_comment' || steps.eval-comment.outcome == 'success'
        shell: bash
        run: |
          # Check one secret as proxy for secrets access (repo has all or none)
          if [[ -n "${{ secrets.AZURE_API_KEY }}" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
          fi

      - name: Post initial comment
        id: initial-comment
        if: steps.check-tests.outputs.should-run == 'true' && steps.eval-params.outputs.pr_number != ''
        uses: actions/github-script@v7
        with:
          script: |
            // SECURITY: Load helpers from trusted checkout (base branch) to prevent code injection from PRs
            const {
              AUTO_EVAL_COMMENT_IDENTIFIER,
              buildParams,
              buildBody,
              buildRerunFooter,
              parseRunHistory,
              extractCurrentRun,
              buildAutoCommentWithHistory
            } = require('./.trusted/.github/scripts/eval-comment-helpers.js');

            const p = buildParams(JSON.parse(${{ toJSON(steps.eval-params.outputs.base_params) }}));
            const progressSteps = [
              [false, 'Setup HolmesGPT environment'],
              [false, 'Collect evals to run'],
              [false, 'Setup KIND cluster'],
              [false, 'Run evals']
            ];

            // For automated runs (not /eval or workflow_dispatch), use persistent comment
            if (!p.isManual) {
              // Find existing persistent comment
              let existingComment = null;
              let page = 1;
              while (true) {
                const response = await github.rest.issues.listComments({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: p.prNumber,
                  per_page: 100,
                  page: page
                });
                const found = response.data.find(c => c.body.includes(AUTO_EVAL_COMMENT_IDENTIFIER));
                if (found) {
                  existingComment = found;
                  break;
                }
                if (response.data.length < 100) break;
                page++;
              }

              const currentContent = buildBody(p, progressSteps, { context });
              const footer = buildRerunFooter(p, context);

              if (existingComment) {
                // Parse history from existing comment, including moving current run to history
                const previousRuns = parseRunHistory(existingComment.body);
                const currentRun = extractCurrentRun(existingComment.body);
                if (currentRun && currentRun.content) {
                  previousRuns.unshift(currentRun);
                }
                const body = buildAutoCommentWithHistory(currentContent, previousRuns, footer);
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: body
                });
                core.setOutput('comment_id', existingComment.id.toString());
                core.setOutput('is_persistent', 'true');
              } else {
                // Create new persistent comment
                const body = buildAutoCommentWithHistory(currentContent, [], footer);
                const comment = await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: p.prNumber,
                  body: body
                });
                core.setOutput('comment_id', comment.data.id.toString());
                core.setOutput('is_persistent', 'true');
              }
            } else {
              // Manual runs: create a fresh comment (original behavior)
              const comment = await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: p.prNumber,
                body: buildBody(p, progressSteps, { context }) + buildRerunFooter(p, context)
              });
              core.setOutput('comment_id', comment.data.id.toString());
              core.setOutput('is_persistent', 'false');
            }

      # Reordered: Setup HolmesGPT first (needed for pytest), then collect evals, then KIND
      - name: Setup HolmesGPT environment
        if: steps.check-tests.outputs.should-run == 'true'
        uses: ./code/.github/actions/setup-holmes-env
        with:
          python-version: '3.12'
          install-kubectl: 'true'
          working-directory: code

      - name: Update progress - HolmesGPT setup done
        if: steps.check-tests.outputs.should-run == 'true' && steps.eval-params.outputs.pr_number != '' && steps.initial-comment.outputs.comment_id != ''
        uses: actions/github-script@v7
        with:
          script: |
            // SECURITY: Load helpers from trusted checkout (base branch) to prevent code injection from PRs
            const {
              buildParams, buildBody, buildRerunFooter, parseRunHistory, buildAutoCommentWithHistory
            } = require('./.trusted/.github/scripts/eval-comment-helpers.js');

            const p = buildParams({
              ...JSON.parse(${{ toJSON(steps.eval-params.outputs.base_params) }}),
              comment_id: ${{ toJSON(steps.initial-comment.outputs.comment_id) }}
            });
            const isPersistent = ${{ toJSON(steps.initial-comment.outputs.is_persistent) }} === 'true';
            const progressSteps = [
              [true, 'Setup HolmesGPT environment'],
              [false, 'Collect evals to run'],
              [false, 'Setup KIND cluster'],
              [false, 'Run evals']
            ];

            let body;
            if (isPersistent) {
              // Preserve history for automated runs
              const existingComment = await github.rest.issues.getComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: p.commentId
              });
              const previousRuns = parseRunHistory(existingComment.data.body);
              const currentContent = buildBody(p, progressSteps, { context });
              const footer = buildRerunFooter(p, context);
              body = buildAutoCommentWithHistory(currentContent, previousRuns, footer);
            } else {
              body = buildBody(p, progressSteps, { context }) + buildRerunFooter(p, context);
            }

            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: p.commentId,
              body: body
            });

      - name: Collect evals to run
        if: steps.check-tests.outputs.should-run == 'true' && steps.eval-params.outputs.pr_number != ''
        id: test-preview
        working-directory: code
        shell: bash
        env:
          MODEL: ${{ steps.eval-params.outputs.model }}
          EVAL_MARKER_EXPR: ${{ steps.eval-params.outputs.marker_expr }}
          EVAL_FILTER: ${{ steps.eval-params.outputs.filter }}
        run: |
          # Unset MODEL if empty so get_models() uses DEFAULT_MODEL
          [[ -z "$MODEL" ]] && unset MODEL

          PYTEST_ARGS=(--collect-only -q tests/llm/test_ask_holmes.py tests/llm/test_investigate.py -m "$EVAL_MARKER_EXPR")
          [[ -n "$EVAL_FILTER" ]] && PYTEST_ARGS+=(-k "$EVAL_FILTER")

          # Collect test names
          TEST_LIST=$(poetry run pytest "${PYTEST_ARGS[@]}" 2>/dev/null | grep -E "^tests/llm/" || true)
          TEST_COUNT=$(echo "$TEST_LIST" | grep -c "^tests/llm/" || true)

          # Collect valid markers from pyproject.toml using Python/tomllib (excluding 'llm' which is internal)
          # Output as comma-separated plain names
          VALID_MARKERS=$(python3 << 'PYEOF'
          import tomllib
          with open('pyproject.toml', 'rb') as f:
              config = tomllib.load(f)
          markers = config.get('tool', {}).get('pytest', {}).get('ini_options', {}).get('markers', [])
          names = [m.split(':')[0] for m in sorted(markers) if m.split(':')[0] != 'llm']
          print(','.join(names))
          PYEOF
          )

          # Output for next steps
          echo "test_count=$TEST_COUNT" >> $GITHUB_OUTPUT

          EOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)
          echo "test_preview<<$EOF" >> $GITHUB_OUTPUT
          echo "$TEST_LIST" >> $GITHUB_OUTPUT
          echo "$EOF" >> $GITHUB_OUTPUT

          EOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)
          echo "valid_markers<<$EOF" >> $GITHUB_OUTPUT
          echo "$VALID_MARKERS" >> $GITHUB_OUTPUT
          echo "$EOF" >> $GITHUB_OUTPUT

      - name: Update progress - evals collected
        if: steps.check-tests.outputs.should-run == 'true' && steps.eval-params.outputs.pr_number != '' && steps.initial-comment.outputs.comment_id != ''
        uses: actions/github-script@v7
        with:
          script: |
            // SECURITY: Load helpers from trusted checkout (base branch) to prevent code injection from PRs
            const {
              buildParams, buildBody, buildRerunFooter, parseRunHistory, buildAutoCommentWithHistory
            } = require('./.trusted/.github/scripts/eval-comment-helpers.js');

            const p = buildParams({
              ...JSON.parse(${{ toJSON(steps.eval-params.outputs.base_params) }}),
              comment_id: ${{ toJSON(steps.initial-comment.outputs.comment_id) }},
              test_count: ${{ toJSON(steps.test-preview.outputs.test_count) }},
              test_preview: ${{ toJSON(steps.test-preview.outputs.test_preview) }},
              valid_markers: ${{ toJSON(steps.test-preview.outputs.valid_markers) }}
            });
            const isPersistent = ${{ toJSON(steps.initial-comment.outputs.is_persistent) }} === 'true';
            const progressSteps = [
              [true, 'Setup HolmesGPT environment'],
              [true, `Collect evals to run (${p.testCount} tests)`],
              [false, 'Setup KIND cluster'],
              [false, 'Run evals']
            ];

            let body;
            if (isPersistent) {
              // Preserve history for automated runs
              const existingComment = await github.rest.issues.getComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: p.commentId
              });
              const previousRuns = parseRunHistory(existingComment.data.body);
              const currentContent = buildBody(p, progressSteps, { testPreview: p.testCount !== '0' ? p.testPreview : null, context });
              const footer = buildRerunFooter(p, context);
              body = buildAutoCommentWithHistory(currentContent, previousRuns, footer);
            } else {
              body = buildBody(p, progressSteps, { testPreview: p.testCount !== '0' ? p.testPreview : null, context }) + buildRerunFooter(p, context);
            }

            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: p.commentId,
              body: body
            });

      - name: Setup KIND cluster
        if: steps.check-tests.outputs.should-run == 'true'
        uses: ./code/.github/actions/setup-kind-cluster
        with:
          cluster-name: 'kind'
          wait-for-ready: 'true'

      - name: Update progress - KIND ready, starting evals
        if: steps.check-tests.outputs.should-run == 'true' && steps.eval-params.outputs.pr_number != '' && steps.initial-comment.outputs.comment_id != ''
        uses: actions/github-script@v7
        with:
          script: |
            // SECURITY: Load helpers from trusted checkout (base branch) to prevent code injection from PRs
            const {
              buildParams, buildBody, buildRerunFooter, parseRunHistory, buildAutoCommentWithHistory
            } = require('./.trusted/.github/scripts/eval-comment-helpers.js');

            const p = buildParams({
              ...JSON.parse(${{ toJSON(steps.eval-params.outputs.base_params) }}),
              comment_id: ${{ toJSON(steps.initial-comment.outputs.comment_id) }},
              test_count: ${{ toJSON(steps.test-preview.outputs.test_count) }},
              test_preview: ${{ toJSON(steps.test-preview.outputs.test_preview) }},
              valid_markers: ${{ toJSON(steps.test-preview.outputs.valid_markers) }}
            });
            const isPersistent = ${{ toJSON(steps.initial-comment.outputs.is_persistent) }} === 'true';
            const progressSteps = [
              [true, 'Setup HolmesGPT environment'],
              [true, `Collect evals to run (${p.testCount} tests)`],
              [true, 'Setup KIND cluster'],
              [false, 'Run evals']
            ];

            let body;
            if (isPersistent) {
              // Preserve history for automated runs
              const existingComment = await github.rest.issues.getComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: p.commentId
              });
              const previousRuns = parseRunHistory(existingComment.data.body);
              const currentContent = buildBody(p, progressSteps, { testPreview: p.testCount !== '0' ? p.testPreview : null, context });
              const footer = buildRerunFooter(p, context);
              body = buildAutoCommentWithHistory(currentContent, previousRuns, footer);
            } else {
              body = buildBody(p, progressSteps, { testPreview: p.testCount !== '0' ? p.testPreview : null, context }) + buildRerunFooter(p, context);
            }

            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: p.commentId,
              body: body
            });

      - name: Run tests
        if: steps.check-tests.outputs.should-run == 'true'
        id: evals
        continue-on-error: true
        working-directory: code
        shell: bash
        env:
          RUN_LIVE: "true"
          AZURE_API_BASE: ${{ secrets.AZURE_API_BASE }}
          AZURE_API_KEY: ${{ secrets.AZURE_API_KEY }}
          AZURE_API_VERSION: ${{ secrets.AZURE_API_VERSION }}
          AWS_BEARER_TOKEN_BEDROCK: ${{ secrets.AWS_BEARER_TOKEN_BEDROCK }}
          AWS_REGION_NAME: ${{ vars.AWS_REGION_NAME }}
          MODEL: ${{ steps.eval-params.outputs.model }}
          ITERATIONS: ${{ steps.eval-params.outputs.iterations }}
          CLASSIFIER_MODEL: ${{ vars.CLASSIFIER_MODEL || 'gpt-4o' }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          ELASTICSEARCH_URL: ${{ secrets.ELASTICSEARCH_URL }}
          ELASTICSEARCH_API_KEY: ${{ secrets.ELASTICSEARCH_API_KEY }}
          BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
          UPLOAD_DATASET: "true"
          EXPERIMENT_ID: github-${{ github.run_id }}.${{ github.run_number }}.${{ github.run_attempt }}
          GENERATE_REGRESSIONS_FILE: "true"
          EVAL_MARKER_EXPR: ${{ steps.eval-params.outputs.marker_expr }}
          EVAL_FILTER: ${{ steps.eval-params.outputs.filter }}
          EVAL_BRANCH: ${{ steps.eval-params.outputs.branch }}
        run: |
          # Unset MODEL if empty so get_models() uses DEFAULT_MODEL
          [[ -z "$MODEL" ]] && unset MODEL

          START_TIME=$(date +%s)

          PYTEST_ARGS=(--no-cov tests/llm/test_ask_holmes.py tests/llm/test_investigate.py -s -n10 -m "$EVAL_MARKER_EXPR")
          [[ -n "$EVAL_FILTER" ]] && PYTEST_ARGS+=(-k "$EVAL_FILTER")
          poetry run pytest "${PYTEST_ARGS[@]}" || true

          DURATION=$(($(date +%s) - START_TIME))
          if [[ $DURATION -ge 3600 ]]; then
            echo "duration=$((DURATION/3600))h $(((DURATION%3600)/60))m $((DURATION%60))s" >> $GITHUB_OUTPUT
          elif [[ $DURATION -ge 60 ]]; then
            echo "duration=$((DURATION/60))m $((DURATION%60))s" >> $GITHUB_OUTPUT
          else
            echo "duration=${DURATION}s" >> $GITHUB_OUTPUT
          fi

      - name: Post evaluation results
        if: always() && steps.eval-params.outputs.pr_number != ''
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            // SECURITY: Load helpers from trusted checkout (base branch) to prevent code injection from PRs
            const {
              buildParams, buildBody, buildRerunFooter, parseRunHistory, buildAutoCommentWithHistory
            } = require('./.trusted/.github/scripts/eval-comment-helpers.js');

            const p = buildParams({
              ...JSON.parse(${{ toJSON(steps.eval-params.outputs.base_params) }}),
              comment_id: ${{ toJSON(steps.initial-comment.outputs.comment_id) }},
              duration: ${{ toJSON(steps.evals.outputs.duration) }},
              valid_markers: ${{ toJSON(steps.test-preview.outputs.valid_markers) }},
              triggered_by: ${{ toJSON(steps.eval-params.outputs.triggered_by) }}
            });
            const isPersistent = ${{ toJSON(steps.initial-comment.outputs.is_persistent) }} === 'true';
            const report = fs.existsSync('code/evals_report.md') ? fs.readFileSync('code/evals_report.md', 'utf8') : '';
            const failures = fs.existsSync('code/regressions.txt') ? fs.readFileSync('code/regressions.txt', 'utf8').trim() : '';
            const hasFailures = failures && failures !== '0';

            // Build notification header for the user who triggered the eval (only for manual runs)
            const triggeredBy = p.triggered_by || '';
            const statusText = hasFailures
              ? `‚ö†Ô∏è Completed with ${failures} failure${failures === '1' ? '' : 's'}`
              : '‚úÖ Completed successfully';
            const notifyHeader = triggeredBy ? `@${triggeredBy} Your eval run has finished. ${statusText}\n\n---\n\n` : '';

            // Build current run content
            let currentContent = buildBody(p, null, {
              icon: p.isManual ? 'üß™' : '‚úÖ',
              title: p.isManual ? 'Manual Eval Results' : 'Results of HolmesGPT evals',
              context
            });

            // Insert failures warning before "Historical Comparison Details" if present
            let reportContent = report || '‚ö†Ô∏è No eval report was generated.\n\n';
            if (hasFailures) {
              const failuresWarning = `\n### ‚ö†Ô∏è ${failures} Failure${failures === '1' ? '' : 's'} Detected\n\n`;
              const historyMarker = '### Historical Comparison Details';
              const historyIdx = reportContent.indexOf(historyMarker);
              if (historyIdx !== -1) {
                // Insert before Historical Comparison Details
                reportContent = reportContent.slice(0, historyIdx) + failuresWarning + reportContent.slice(historyIdx);
              } else {
                // Append at the end if no Historical Comparison Details section
                reportContent += failuresWarning;
              }
            }
            currentContent += '\n' + reportContent;
            const footer = buildRerunFooter(p, context, { includeLegend: true });

            if (isPersistent && p.commentId) {
              // For automated runs: update the persistent comment, preserving history
              const existingComment = await github.rest.issues.getComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: p.commentId
              });
              const previousRuns = parseRunHistory(existingComment.data.body);
              const body = buildAutoCommentWithHistory(currentContent, previousRuns, footer);
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: p.commentId,
                body: body
              });
            } else {
              // For manual runs: delete progress comment and create fresh results comment
              // This ensures the user gets a GitHub notification when evals complete
              const body = notifyHeader + currentContent + footer;
              if (p.commentId) {
                await github.rest.issues.deleteComment({
                  owner: context.repo.owner, repo: context.repo.repo,
                  comment_id: p.commentId
                });
              }
              await github.rest.issues.createComment({
                owner: context.repo.owner, repo: context.repo.repo,
                issue_number: p.prNumber, body
              });
            }

      - name: Add completion reaction
        if: always() && github.event_name == 'issue_comment' && steps.eval-comment.outcome == 'success'
        uses: actions/github-script@v7
        with:
          script: |
            // Remove eyes reaction (was added to indicate processing started)
            const reactions = await github.rest.reactions.listForIssueComment({
              owner: context.repo.owner, repo: context.repo.repo,
              comment_id: context.payload.comment.id
            });
            const eyesReaction = reactions.data.find(r => r.content === 'eyes' && r.user.login === 'github-actions[bot]');
            if (eyesReaction) {
              await github.rest.reactions.deleteForIssueComment({
                owner: context.repo.owner, repo: context.repo.repo,
                comment_id: context.payload.comment.id, reaction_id: eyesReaction.id
              });
            }
            // Add hooray reaction to indicate completion
            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner, repo: context.repo.repo,
              comment_id: context.payload.comment.id, content: 'hooray'
            });

      - name: Check test results
        if: always()
        working-directory: code
        run: |
          if [[ -f "regressions.txt" ]]; then
            echo "‚ö†Ô∏è There are failures in the evals. Please check the file for details."
            cat regressions.txt
          else
            echo "‚úÖ All tests passed."
          fi
